{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7696618 , -0.6384518 ,  0.22815812], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('Pendulum-v1')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        return state, reward, done, info\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d414446684641579eaf65a35d409c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: \n",
       "`np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
       "  if not isinstance(terminated, (bool, np.bool8)):\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: \n",
       "`np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
       "  if not isinstance(terminated, (bool, np.bool8)):\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:278: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x26df51c1970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "#训练一个模型\n",
    "model = PPO('MlpPolicy', env, verbose=0).learn(8000, progress_bar=True)\n",
    "\n",
    "#保存模型\n",
    "model.save('models/save')\n",
    "\n",
    "#加载模型\n",
    "model = PPO.load('models/save')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PPO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#加载模型\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/save\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#如果要继续训练,需要重新给它一个env,因为env在保存模型时是不能保存下来的\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PPO' is not defined"
     ]
    }
   ],
   "source": [
    "#加载模型\n",
    "model = PPO.load('models/save', verbose=0)\n",
    "\n",
    "#如果要继续训练,需要重新给它一个env,因为env在保存模型时是不能保存下来的\n",
    "model.set_env(env)\n",
    "\n",
    "#继续训练\n",
    "model.learn(8000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MyWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>\n",
      "0 [0.07218624 0.99739116 0.8919712 ] [1.7378713] -2.3282253153208705\n",
      "20 [-0.2385203 -0.9711375 -1.170521 ] [-1.1149726] -3.4202881012693864\n",
      "40 [-0.55694973  0.8305462   2.9121382 ] [-1.5324578] -5.5225072331571035\n",
      "60 [-0.984551   -0.17509787 -4.571843  ] [0.6415151] -10.88529545283633\n",
      "80 [-0.9633814  -0.26813474  3.9805312 ] [0.21465972] -9.822191331867616\n",
      "100 [-0.6252558  0.7804199 -3.279739 ] [-1.7166805] -6.124280065923935\n",
      "120 [-0.18435736 -0.98285925  2.4722273 ] [1.0108101] -3.696501197218173\n",
      "140 [-0.42587793  0.9047806  -3.044164  ] [-0.9456013] -4.970615957293665\n",
      "160 [-0.15890619 -0.98729366  2.3169239 ] [-0.47413313] -3.53124982863186\n",
      "180 [ 0.25435236  0.96711165 -2.3977344 ] [-1.6481655] -2.303222614544829\n",
      "200 [-0.11203683 -0.9937041   3.6140094 ] [0.53798527] -4.139116616997664\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# 测试一个环境\n",
    "def test(env, wrap_action_in_list=False):\n",
    "    print(env)\n",
    "\n",
    "    state = env.reset()\n",
    "    over = False\n",
    "    step = 0\n",
    "\n",
    "    while not over:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        if wrap_action_in_list:\n",
    "            action = action\n",
    "\n",
    "        next_state, reward, over,_ = env.step(action)\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(step, state, action, reward)\n",
    "\n",
    "        if step > 200:\n",
    "            break\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "test(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StepLimitWrapper<MyWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>>\n",
      "0 [-0.9987958  -0.04906001 -0.0384343 ] [0.16010138] -9.563809677068514\n",
      "20 [-0.99414724  0.10803361  0.42362726] [1.9890343] -9.223101039549428\n",
      "40 [-0.99859345  0.05301971 -1.004939  ] [-1.1963421] -9.64155064102953\n",
      "60 [-0.92510855 -0.37970266  1.4664237 ] [1.6687284] -7.791976793563201\n",
      "80 [-0.812863    0.5824549  -0.16073538] [-1.1355683] -6.353502565970594\n"
     ]
    }
   ],
   "source": [
    "# 修改最大步数\n",
    "class StepLimitWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        # 修改done字段\n",
    "        if self.current_step >= 100:\n",
    "            done = True\n",
    "        \n",
    "        return state, reward, done, info\n",
    "\n",
    "test(StepLimitWrapper(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NormalizeActionWrapper<MyWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>>\n",
      "0 [ 0.82443213 -0.5659608   0.64134234] [0.35957316] -0.40356963721489625\n",
      "20 [-0.44379258  0.89612955 -6.6596518 ] [0.76076335] -8.560839787785925\n",
      "40 [-0.9974366   0.07155585  6.7546034 ] [0.26507068] -13.987498185926063\n",
      "60 [ 0.5134467  -0.85812145 -2.550399  ] [-0.68042195] -1.7165035126270012\n",
      "80 [ 0.81502736  0.5794225  -0.95199615] [-0.7183617] -0.474642456584224\n",
      "100 [-0.30434722 -0.95256114  6.379881  ] [0.8711266] -7.6079090172004875\n",
      "120 [ 0.6894555  -0.72432804 -2.7196856 ] [0.9247283] -1.3992853754728931\n",
      "140 [ 0.75123686  0.6600327  -1.3111333 ] [0.25518644] -0.6918099804453607\n",
      "160 [ 0.33620146 -0.9417901   4.513786  ] [-0.09290681] -3.5452375840404002\n",
      "180 [-0.02041132 -0.9997917  -4.9825    ] [0.35290998] -5.014975330051451\n",
      "200 [0.6653459  0.74653524 0.72290903] [0.04104136] -0.7626445313694492\n"
     ]
    }
   ],
   "source": [
    "# 修改环境的动作空间\n",
    "import numpy as np\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # 获取动作空间\n",
    "        action_space = env.action_space\n",
    "\n",
    "        # 动作空间必须是连续值\n",
    "        assert isinstance(action_space, gym.spaces.Box)\n",
    "\n",
    "        # 重新定义动作空间，在 -1,+1之间的连续值\n",
    "        # 这里其实只影响env.action_space.sample的返回结果\n",
    "        # 实际计算时，还是正负2之间计算的\n",
    "        env.action_space = gym.spaces.Box(low=-1, high=1,shape=action_space.shape,dtype=np.float32)\n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # 重新缩放动作的值域\n",
    "        action = action * 2.0\n",
    "\n",
    "        action = min(action, 2.0)\n",
    "        action = max(action, -2.0)\n",
    "\n",
    "        return self.env.step(action)\n",
    "\n",
    "test(NormalizeActionWrapper(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StateStepWrapper<MyWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>>\n",
      "0 [-0.08650355 -0.99625152 -0.19312151  0.        ] [0.33387163] -2.75084274915039\n",
      "20 [-0.47532126  0.8798123   1.78159261  0.2       ] [1.1249055] -4.587547091643162\n",
      "40 [-0.62708122 -0.77895391 -3.54702616  0.4       ] [1.2109034] -6.315793315379384\n",
      "60 [-0.98780531  0.15569402  3.87362218  0.6       ] [1.416071] -10.41429326941321\n",
      "80 [-0.94566798  0.32513383 -4.14462137  0.8       ] [0.353936] -9.616483504146965\n",
      "100 [-0.6705361  -0.7418769   3.86490726  1.        ] [-1.5554457] -6.812549539032869\n",
      "120 [-0.53836179  0.84271383 -2.25301814  1.2       ] [-0.7851366] -5.084779853345219\n",
      "140 [-0.53221834 -0.84660715  1.04854572  1.4       ] [-0.77876776] -4.656039144513375\n",
      "160 [-0.66517442  0.74668801  1.29779148  1.6       ] [-1.647633] -5.454352129905857\n",
      "180 [-0.79891515 -0.60144377 -2.9577992   1.8       ] [0.35013768] -7.106422011578631\n",
      "200 [-0.98384923  0.17899917  4.81690931  2.        ] [0.920967] -11.092323610817305\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 修改状态空间\n",
    "class StateStepWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # 状态空间必须是连续值\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "\n",
    "        # 增加一个新状态字段\n",
    "        low = np.concatenate([env.observation_space.low, [0.0]])\n",
    "        high = np.concatenate([env.observation_space.high, [1.0]])\n",
    "\n",
    "        env.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "        self.step_current = 0\n",
    "    def reset(self):\n",
    "        self.step_current = 0\n",
    "        return np.concatenate([self.env.reset(), [0.0]])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_current += 1\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        return self.get_state(state), reward, done, info\n",
    "    \n",
    "    def get_state(self, state):\n",
    "        # 添加一个新的字段\n",
    "        state_step = self.step_current / 100\n",
    "        return np.concatenate([state, [state_step]]) # 加一列\n",
    "\n",
    "test(StateStepWrapper(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49796903,  0.86719483, -0.9840721 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "class Pendulum(gym.Wrapper):\n",
    "    def __init__(self):\n",
    "        env = gym.make(\"Pendulum-v1\")\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "    \n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        return state, reward, done, info\n",
    "\n",
    "Pendulum().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 115      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | -0.0431  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -37.5    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 832      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.000636 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -30.6    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 917      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x244d67bd2e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SB3内置的Wrapper:Monitor\n",
    "# 训练中增加一些日志\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: Monitor(Pendulum())])\n",
    "\n",
    "\n",
    "A2C('MlpPolicy', env, verbose=1).learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x00000244D7812670>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([Pendulum])\n\u001b[0;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m VecNormalize(env)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(env, wrap_action_in_list)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap_action_in_list:\n\u001b[0;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m---> 18\u001b[0m next_state, reward, over,_ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step, state, action, reward)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_normalize.py:178\u001b[0m, in \u001b[0;36mVecNormalize.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m    172\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[39m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     obs, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_obs \u001b[39m=\u001b[39m obs\n\u001b[0;32m    180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_reward \u001b[39m=\u001b[39m rewards\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mPendulum.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m---> 14\u001b[0m     state, reward, done, _, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, reward, done, info\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    215\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    216\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[0;32m    217\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\gym\\envs\\classic_control\\pendulum.py:127\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    124\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml\n\u001b[0;32m    125\u001b[0m dt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt\n\u001b[1;32m--> 127\u001b[0m u \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mclip(u, \u001b[39m-\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_torque, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_torque)[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_u \u001b[39m=\u001b[39m u  \u001b[39m# for rendering\u001b[39;00m\n\u001b[0;32m    129\u001b[0m costs \u001b[39m=\u001b[39m angle_normalize(th) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m thdot\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m (u\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# SB3内置的Warpper\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack,DummyVecEnv\n",
    "\n",
    "# VecNormalize,他会对state和reward进行Normalize\n",
    "env = DummyVecEnv([Pendulum])\n",
    "env = VecNormalize(env)\n",
    "\n",
    "test(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17680b14d7d9a3bae9c4871fddb2c879681776f38d67c6256fcb9de429c7e820"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
