{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Discrete(2)\n",
      "195.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n",
      "reward:  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "print(env.observation_space) # 观测空间\n",
    "print(env.action_space) # 动作空间\n",
    "print(env.spec.reward_threshold) # 成功求解的阈值\n",
    " \n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # agent policy that uses the obs and info\n",
    "    obs, r, done, info = env.step(action)\n",
    "\n",
    "    print(\"reward: \", r)\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 32.8     |\n",
      "|    ep_rew_mean        | 32.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.623   |\n",
      "|    explained_variance | 0.117    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.13     |\n",
      "|    value_loss         | 18.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 40.5     |\n",
      "|    ep_rew_mean        | 40.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1357     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.548   |\n",
      "|    explained_variance | 0.139    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.49     |\n",
      "|    value_loss         | 7.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 48.6     |\n",
      "|    ep_rew_mean        | 48.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1372     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.567   |\n",
      "|    explained_variance | 0.0233   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.43     |\n",
      "|    value_loss         | 6.05     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 68.3     |\n",
      "|    ep_rew_mean        | 68.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1390     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.604   |\n",
      "|    explained_variance | 0.00221  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.06     |\n",
      "|    value_loss         | 5.3      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 93.3     |\n",
      "|    ep_rew_mean        | 93.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1393     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.618   |\n",
      "|    explained_variance | 0.00272  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.01     |\n",
      "|    value_loss         | 4.72     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 108       |\n",
      "|    ep_rew_mean        | 108       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1398      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.6      |\n",
      "|    explained_variance | -0.000954 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 0.922     |\n",
      "|    value_loss         | 4.19      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 121       |\n",
      "|    ep_rew_mean        | 121       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1403      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.53     |\n",
      "|    explained_variance | -0.000423 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 1.27      |\n",
      "|    value_loss         | 3.66      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 140      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1405     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.584   |\n",
      "|    explained_variance | 0.00067  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.917    |\n",
      "|    value_loss         | 3.17     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 160      |\n",
      "|    ep_rew_mean        | 160      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1409     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0.000352 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    value_loss         | 2.73     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 172       |\n",
      "|    ep_rew_mean        | 172       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1415      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.488    |\n",
      "|    explained_variance | -0.000121 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.84      |\n",
      "|    value_loss         | 2.33      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 190      |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1412     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.587   |\n",
      "|    explained_variance | 6.06e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.669    |\n",
      "|    value_loss         | 1.94     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 213       |\n",
      "|    ep_rew_mean        | 213       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1417      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.541    |\n",
      "|    explained_variance | -0.000165 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.465     |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "env.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17680b14d7d9a3bae9c4871fddb2c879681776f38d67c6256fcb9de429c7e820"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
